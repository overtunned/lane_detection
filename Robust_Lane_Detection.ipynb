{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Robust Lane Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/overtunned/lane_detection/blob/main/Robust_Lane_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz1MA2MvBzQ2",
        "outputId": "1d0906b3-a1cb-4b94-b468-0b9be429f18b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSxOvrjlRH1k",
        "outputId": "a30f38e5-474c-4238-c84e-615f1a67b5bf"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/robust-lane-detection-main/LaneDetectionCode"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/robust-lane-detection-main/LaneDetectionCode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "8cWOKVgZ2hpK",
        "cellView": "form"
      },
      "source": [
        "#@title walk\n",
        "import os\n",
        "for dirname, i, filenames in os.walk('/content/Robust-Lane-Detection/LaneDetectionCode'):\n",
        "  print(dirname,i,filenames)\n",
        "  for filename in filenames:\n",
        "    print(os.path.join(dirname, filename))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NWXKztrJ82R",
        "cellView": "form"
      },
      "source": [
        "#@title Config.py\n",
        "import argparse\n",
        "\n",
        "# globel param\n",
        "# dataset setting\n",
        "img_width = 256\n",
        "img_height = 128\n",
        "img_channel = 3\n",
        "label_width = 256\n",
        "label_height = 128\n",
        "label_channel = 1\n",
        "data_loader_numworkers = 8\n",
        "class_num = 2\n",
        "\n",
        "# path\n",
        "train_path = \"./data/train_index.txt\"\n",
        "val_path = \"./data/val_index.txt\"\n",
        "test_path = \"./data/test_index_0530.txt\"\n",
        "save_path = \"./save/result/\"\n",
        "pretrained_path='./pretrained/unetlstm.pth'\n",
        "\n",
        "# weight\n",
        "class_weight = [0.02, 1.02]\n",
        "\n",
        "def args_setting():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description='PyTorch UNet-ConvLSTM')\n",
        "    parser.add_argument('--model',type=str, default='UNet-ConvLSTM',help='( UNet-ConvLSTM | SegNet-ConvLSTM | UNet | SegNet | ')\n",
        "    parser.add_argument('--batch-size', type=int, default=15, metavar='N',\n",
        "                        help='input batch size for training (default: 10)')\n",
        "    parser.add_argument('--test-batch-size', type=int, default=1, metavar='N',\n",
        "                        help='input batch size for testing (default: 100)')\n",
        "    parser.add_argument('--epochs', type=int, default=30, metavar='N',\n",
        "                        help='number of epochs to train (default: 30)')\n",
        "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
        "                        help='learning rate (default: 0.01)')\n",
        "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
        "                        help='SGD momentum (default: 0.5)')\n",
        "    parser.add_argument('--cuda', action='store_true', default=True,\n",
        "                        help='use CUDA training')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                        help='how many batches to wait before logging training status')\n",
        "    args = parser.parse_args(args=[])\n",
        "    return args"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hsRRPGPICb3",
        "cellView": "form"
      },
      "source": [
        "#@title Dataset.py\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def readTxt(file_path):\n",
        "    img_list = []\n",
        "    with open(file_path, 'r') as file_to_read:\n",
        "        while True:\n",
        "            lines = file_to_read.readline()\n",
        "            if not lines:\n",
        "                break\n",
        "            item = lines.strip().split()\n",
        "            path = ['.'+i[21:] for i in item]\n",
        "            img_list.append(path)\n",
        "    file_to_read.close()\n",
        "    return img_list\n",
        "\n",
        "class RoadSequenceDataset(Dataset):\n",
        "\n",
        "    def __init__(self, file_path, transforms):\n",
        "\n",
        "        self.img_list = readTxt(file_path)\n",
        "        self.dataset_size = len(self.img_list)\n",
        "        self.transforms = transforms\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path_list = self.img_list[idx]\n",
        "        data = Image.open(img_path_list[4])\n",
        "        label = Image.open(img_path_list[5])\n",
        "        data = self.transforms(data)\n",
        "        label = torch.squeeze(self.transforms(label))\n",
        "        sample = {'data': data, 'label': label}\n",
        "        return sample\n",
        "\n",
        "class RoadSequenceDatasetList(Dataset):\n",
        "\n",
        "    def __init__(self, file_path, transforms):\n",
        "\n",
        "        self.img_list = readTxt(file_path)\n",
        "        self.dataset_size = len(self.img_list)\n",
        "        self.transforms = transforms\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path_list = self.img_list[idx]\n",
        "        data = []\n",
        "        for i in range(5):\n",
        "            data.append(torch.unsqueeze(self.transforms(Image.open(img_path_list[i])), dim=0))\n",
        "        data = torch.cat(data, 0)\n",
        "        label = Image.open(img_path_list[5])\n",
        "        label = torch.squeeze(self.transforms(label))\n",
        "        sample = {'data': data, 'label': label}\n",
        "        return sample\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "uAsh_zHhKoYi"
      },
      "source": [
        "#@title utils.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            double_conv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "\n",
        "        #  would be a nice idea if the upsampling could be learned too,\n",
        "        #  but my machine do not have enough memory to handle all those weights\n",
        "        if bilinear:\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffX = x1.size()[2] - x2.size()[2]\n",
        "        diffY = x1.size()[3] - x2.size()[3]\n",
        "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
        "                        diffY // 2, int(diffY / 2)))\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias):\n",
        "        \"\"\"\n",
        "        Initialize ConvLSTM cell.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_size: (int, int)\n",
        "            Height and width of input tensor as (height, width).\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.height, self.width = input_size\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.bias = bias\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
        "                              out_channels=4 * self.hidden_dim,\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
        "\n",
        "        combined_conv = self.conv(combined)\n",
        "\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(batch_size, self.hidden_dim, self.height, self.width).cuda(),\n",
        "                torch.zeros(batch_size, self.hidden_dim, self.height, self.width).cuda())\n",
        "\n",
        "\n",
        "class ConvLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n",
        "                 batch_first=False, bias=True, return_all_layers=False):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "\n",
        "        self._check_kernel_size_consistency(kernel_size)\n",
        "\n",
        "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
        "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
        "            raise ValueError('Inconsistent list length.')\n",
        "\n",
        "        self.height, self.width = input_size\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "\n",
        "        cell_list = []\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
        "\n",
        "            cell_list.append(ConvLSTMCell(input_size=(self.height, self.width),\n",
        "                                          input_dim=cur_input_dim,\n",
        "                                          hidden_dim=self.hidden_dim[i],\n",
        "                                          kernel_size=self.kernel_size[i],\n",
        "                                          bias=self.bias))\n",
        "\n",
        "        self.cell_list = nn.ModuleList(cell_list)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_state=None):\n",
        "        \"\"\"\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_tensor: todo\n",
        "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
        "        hidden_state: todo\n",
        "            None. todo implement stateful\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        last_state_list, layer_output\n",
        "        \"\"\"\n",
        "        if not self.batch_first:\n",
        "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
        "            input_tensor=input_tensor.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        # Implement stateful ConvLSTM\n",
        "        if hidden_state is not None:\n",
        "            raise NotImplementedError()\n",
        "        else:\n",
        "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n",
        "\n",
        "        layer_output_list = []\n",
        "        last_state_list = []\n",
        "\n",
        "        seq_len = input_tensor.size(1)\n",
        "        cur_layer_input = input_tensor\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "\n",
        "            h, c = hidden_state[layer_idx]\n",
        "            output_inner = []\n",
        "            for t in range(seq_len):\n",
        "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
        "                                                 cur_state=[h, c])\n",
        "\n",
        "\n",
        "                output_inner.append(h)\n",
        "\n",
        "            layer_output = torch.stack(output_inner, dim=1)\n",
        "            cur_layer_input = layer_output\n",
        "\n",
        "            layer_output = layer_output.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "            layer_output_list.append(layer_output)\n",
        "            last_state_list.append([h, c])\n",
        "\n",
        "        if not self.return_all_layers:\n",
        "            layer_output_list = layer_output_list[-1:]\n",
        "            last_state_list = last_state_list[-1:]\n",
        "\n",
        "        return layer_output_list, last_state_list\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
        "        return init_states\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_kernel_size_consistency(kernel_size):\n",
        "        if not (isinstance(kernel_size, tuple) or\n",
        "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
        "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
        "\n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7NwhysmJ6oI",
        "cellView": "form"
      },
      "source": [
        "#@title model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "import operator\n",
        "\n",
        "def generate_model(args):\n",
        "\n",
        "    use_cuda = args.cuda and torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    assert args.model in [ 'UNet-ConvLSTM', 'SegNet-ConvLSTM', 'UNet', 'SegNet']\n",
        "    if args.model == 'SegNet-ConvLSTM':\n",
        "        model = SegNet_ConvLSTM().to(device)\n",
        "    elif args.model == 'SegNet':\n",
        "        model = SegNet().to(device)\n",
        "    elif args.model == 'UNet-ConvLSTM':\n",
        "        model =UNet_ConvLSTM(img_channel, class_num).to(device)\n",
        "    elif args.model == 'UNet':\n",
        "        model = UNet(img_channel, class_num).to(device)\n",
        "    return model\n",
        "\n",
        "class UNet_ConvLSTM(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet_ConvLSTM, self).__init__()\n",
        "        self.inc = inconv(n_channels, 64)\n",
        "        self.down1 = down(64, 128)\n",
        "        self.down2 = down(128, 256)\n",
        "        self.down3 = down(256, 512)\n",
        "        self.down4 = down(512, 512)\n",
        "        self.up1 = up(1024, 256)\n",
        "        self.up2 = up(512, 128)\n",
        "        self.up3 = up(256, 64)\n",
        "        self.up4 = up(128, 64)\n",
        "        self.outc = outconv(64, n_classes)\n",
        "        self.convlstm = ConvLSTM(input_size=(8,16),\n",
        "                                 input_dim=512,\n",
        "                                 hidden_dim=[512, 512],\n",
        "                                 kernel_size=(3,3),\n",
        "                                 num_layers=2,\n",
        "                                 batch_first=False,\n",
        "                                 bias=True,\n",
        "                                 return_all_layers=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.unbind(x, dim=1)\n",
        "        data = []\n",
        "        for item in x:\n",
        "            x1 = self.inc(item)\n",
        "            x2 = self.down1(x1)\n",
        "            x3 = self.down2(x2)\n",
        "            x4 = self.down3(x3)\n",
        "            x5 = self.down4(x4)\n",
        "            data.append(x5.unsqueeze(0))\n",
        "        data = torch.cat(data, dim=0)\n",
        "        lstm, _ = self.convlstm(data)\n",
        "        test = lstm[0][ -1,:, :, :, :]\n",
        "        x = self.up1(test, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x, test\n",
        "\n",
        "\n",
        "class SegNet_ConvLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SegNet_ConvLSTM,self).__init__()\n",
        "        self.vgg16_bn = models.vgg16_bn(pretrained=True).features\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.index_MaxPool = nn.MaxPool2d(kernel_size=2, stride=2,return_indices=True)\n",
        "        self.index_UnPool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        # net struct\n",
        "        self.conv1_block = nn.Sequential(self.vgg16_bn[0],  # conv2d(3,64,(3,3))\n",
        "                                         self.vgg16_bn[1],  # bn(64,eps=1e-05,momentum=0.1,affine=True)\n",
        "                                         self.vgg16_bn[2],  # relu(in_place)\n",
        "                                         self.vgg16_bn[3],  # conv2d(3,64,(3,3))\n",
        "                                         self.vgg16_bn[4],  # bn(64,eps=1e-05,momentum=0.1,affine=True)\n",
        "                                         self.vgg16_bn[5]   # relu(in_place)\n",
        "                                         )\n",
        "        self.conv2_block = nn.Sequential(self.vgg16_bn[7],\n",
        "                                         self.vgg16_bn[8],\n",
        "                                         self.vgg16_bn[9],\n",
        "                                         self.vgg16_bn[10],\n",
        "                                         self.vgg16_bn[11],\n",
        "                                         self.vgg16_bn[12]\n",
        "                                         )\n",
        "        self.conv3_block = nn.Sequential(self.vgg16_bn[14],\n",
        "                                         self.vgg16_bn[15],\n",
        "                                         self.vgg16_bn[16],\n",
        "                                         self.vgg16_bn[17],\n",
        "                                         self.vgg16_bn[18],\n",
        "                                         self.vgg16_bn[19],\n",
        "                                         self.vgg16_bn[20],\n",
        "                                         self.vgg16_bn[21],\n",
        "                                         self.vgg16_bn[22]\n",
        "                                         )\n",
        "        self.conv4_block = nn.Sequential(self.vgg16_bn[24],\n",
        "                                         self.vgg16_bn[25],\n",
        "                                         self.vgg16_bn[26],\n",
        "                                         self.vgg16_bn[27],\n",
        "                                         self.vgg16_bn[28],\n",
        "                                         self.vgg16_bn[29],\n",
        "                                         self.vgg16_bn[30],\n",
        "                                         self.vgg16_bn[31],\n",
        "                                         self.vgg16_bn[32]\n",
        "                                         )\n",
        "        self.conv5_block = nn.Sequential(self.vgg16_bn[34],\n",
        "                                         self.vgg16_bn[35],\n",
        "                                         self.vgg16_bn[36],\n",
        "                                         self.vgg16_bn[37],\n",
        "                                         self.vgg16_bn[38],\n",
        "                                         self.vgg16_bn[39],\n",
        "                                         self.vgg16_bn[40],\n",
        "                                         self.vgg16_bn[41],\n",
        "                                         self.vgg16_bn[42]\n",
        "                                         )\n",
        "\n",
        "        self.upconv5_block = nn.Sequential(\n",
        "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1, 1)),\n",
        "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           )\n",
        "        self.upconv4_block = nn.Sequential(\n",
        "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(512, 256, kernel_size=(3, 3), padding=(1, 1)),\n",
        "                                           nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           )\n",
        "        self.upconv3_block = nn.Sequential(\n",
        "                                           nn.Conv2d(256, 256, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(256, 256, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(256, 128, kernel_size=(3, 3), padding=(1, 1)),\n",
        "                                           nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           )\n",
        "        self.upconv2_block = nn.Sequential(\n",
        "                                           nn.Conv2d(128, 128, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(128, 64, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu\n",
        "                                           )\n",
        "        self.upconv1_block = nn.Sequential(\n",
        "                                           nn.Conv2d(64, 64, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(64, class_num, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           )\n",
        "        self.convlstm = ConvLSTM(input_size=(4,8),\n",
        "                                 input_dim=512,\n",
        "                                 hidden_dim=[512, 512],\n",
        "                                 kernel_size=(3,3),\n",
        "                                 num_layers=2,\n",
        "                                 batch_first=False,\n",
        "                                 bias=True,\n",
        "                                 return_all_layers=False)\n",
        "    def forward(self, x):\n",
        "        x = torch.unbind(x, dim=1)\n",
        "        data = []\n",
        "        for item in x:\n",
        "            f1, idx1 = self.index_MaxPool(self.conv1_block(item))\n",
        "            f2, idx2 = self.index_MaxPool(self.conv2_block(f1))\n",
        "            f3, idx3 = self.index_MaxPool(self.conv3_block(f2))\n",
        "            f4, idx4 = self.index_MaxPool(self.conv4_block(f3))\n",
        "            f5, idx5 = self.index_MaxPool(self.conv5_block(f4))\n",
        "            data.append(f5.unsqueeze(0))\n",
        "        data = torch.cat(data, dim=0)\n",
        "        lstm, _ = self.convlstm(data)\n",
        "        test = lstm[0][-1,:,:,:,:]\n",
        "        up6 = self.index_UnPool(test,idx5)\n",
        "        up5 = self.index_UnPool(self.upconv5_block(up6), idx4)\n",
        "        up4 = self.index_UnPool(self.upconv4_block(up5), idx3)\n",
        "        up3 = self.index_UnPool(self.upconv3_block(up4), idx2)\n",
        "        up2 = self.index_UnPool(self.upconv2_block(up3), idx1)\n",
        "        up1 = self.upconv1_block(up2)\n",
        "        return F.log_softmax(up1, dim=1)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = inconv(n_channels, 64)\n",
        "        self.down1 = down(64, 128)\n",
        "        self.down2 = down(128, 256)\n",
        "        self.down3 = down(256, 512)\n",
        "        self.down4 = down(512, 512)\n",
        "        self.up1 = up(1024, 256)\n",
        "        self.up2 = up(512, 128)\n",
        "        self.up3 = up(256, 64)\n",
        "        self.up4 = up(128, 64)\n",
        "        self.outc = outconv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SegNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SegNet,self).__init__()\n",
        "        self.vgg16_bn = models.vgg16_bn(pretrained=True).features\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.index_MaxPool = nn.MaxPool2d(kernel_size=2, stride=2,return_indices=True)\n",
        "        self.index_UnPool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        # net struct\n",
        "        self.conv1_block = nn.Sequential(self.vgg16_bn[0],  # conv2d(3,64,(3,3))\n",
        "                                         self.vgg16_bn[1],  # bn(64,eps=1e-05,momentum=0.1,affine=True)\n",
        "                                         self.vgg16_bn[2],  # relu(in_place)\n",
        "                                         self.vgg16_bn[3],  # conv2d(3,64,(3,3))\n",
        "                                         self.vgg16_bn[4],  # bn(64,eps=1e-05,momentum=0.1,affine=True)\n",
        "                                         self.vgg16_bn[5]   # relu(in_place)\n",
        "                                         )\n",
        "        self.conv2_block = nn.Sequential(self.vgg16_bn[7],\n",
        "                                         self.vgg16_bn[8],\n",
        "                                         self.vgg16_bn[9],\n",
        "                                         self.vgg16_bn[10],\n",
        "                                         self.vgg16_bn[11],\n",
        "                                         self.vgg16_bn[12]\n",
        "                                         )\n",
        "        self.conv3_block = nn.Sequential(self.vgg16_bn[14],\n",
        "                                         self.vgg16_bn[15],\n",
        "                                         self.vgg16_bn[16],\n",
        "                                         self.vgg16_bn[17],\n",
        "                                         self.vgg16_bn[18],\n",
        "                                         self.vgg16_bn[19],\n",
        "                                         self.vgg16_bn[20],\n",
        "                                         self.vgg16_bn[21],\n",
        "                                         self.vgg16_bn[22]\n",
        "                                         )\n",
        "        self.conv4_block = nn.Sequential(self.vgg16_bn[24],\n",
        "                                         self.vgg16_bn[25],\n",
        "                                         self.vgg16_bn[26],\n",
        "                                         self.vgg16_bn[27],\n",
        "                                         self.vgg16_bn[28],\n",
        "                                         self.vgg16_bn[29],\n",
        "                                         self.vgg16_bn[30],\n",
        "                                         self.vgg16_bn[31],\n",
        "                                         self.vgg16_bn[32]\n",
        "                                         )\n",
        "        self.conv5_block = nn.Sequential(self.vgg16_bn[34],\n",
        "                                         self.vgg16_bn[35],\n",
        "                                         self.vgg16_bn[36],\n",
        "                                         self.vgg16_bn[37],\n",
        "                                         self.vgg16_bn[38],\n",
        "                                         self.vgg16_bn[39],\n",
        "                                         self.vgg16_bn[40],\n",
        "                                         self.vgg16_bn[41],\n",
        "                                         self.vgg16_bn[42]\n",
        "                                         )\n",
        "\n",
        "        self.upconv5_block = nn.Sequential(\n",
        "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1, 1)),\n",
        "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           )\n",
        "        self.upconv4_block = nn.Sequential(\n",
        "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(512, 256, kernel_size=(3, 3), padding=(1, 1)),\n",
        "                                           nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           )\n",
        "        self.upconv3_block = nn.Sequential(\n",
        "                                           nn.Conv2d(256, 256, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(256, 256, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(256, 128, kernel_size=(3, 3), padding=(1, 1)),\n",
        "                                           nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           )\n",
        "        self.upconv2_block = nn.Sequential(\n",
        "                                           nn.Conv2d(128, 128, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(128, 64, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu\n",
        "                                           )\n",
        "        self.upconv1_block = nn.Sequential(\n",
        "                                           nn.Conv2d(64, 64, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True),\n",
        "                                           self.relu,\n",
        "                                           nn.Conv2d(64, class_num, kernel_size=(3, 3), padding=(1,1)),\n",
        "                                           )\n",
        "    def forward(self, x):\n",
        "        f1, idx1 = self.index_MaxPool(self.conv1_block(x))\n",
        "        f2, idx2 = self.index_MaxPool(self.conv2_block(f1))\n",
        "        f3, idx3 = self.index_MaxPool(self.conv3_block(f2))\n",
        "        f4, idx4 = self.index_MaxPool(self.conv4_block(f3))\n",
        "        f5, idx5 = self.index_MaxPool(self.conv5_block(f4))\n",
        "        up6 = self.index_UnPool(f5,idx5)\n",
        "        up5 = self.index_UnPool(self.upconv5_block(up6), idx4)\n",
        "        up4 = self.index_UnPool(self.upconv4_block(up5), idx3)\n",
        "        up3 = self.index_UnPool(self.upconv3_block(up4), idx2)\n",
        "        up2 = self.index_UnPool(self.upconv2_block(up3), idx1)\n",
        "        up1 = self.upconv1_block(up2)\n",
        "\n",
        "        return F.log_softmax(up1, dim=1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gdz3r3saK4UZ"
      },
      "source": [
        "#@title tools.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            double_conv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "\n",
        "        #  would be a nice idea if the upsampling could be learned too,\n",
        "        #  but my machine do not have enough memory to handle all those weights\n",
        "        if bilinear:\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffX = x1.size()[2] - x2.size()[2]\n",
        "        diffY = x1.size()[3] - x2.size()[3]\n",
        "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
        "                        diffY // 2, int(diffY / 2)))\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias):\n",
        "        \"\"\"\n",
        "        Initialize ConvLSTM cell.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_size: (int, int)\n",
        "            Height and width of input tensor as (height, width).\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.height, self.width = input_size\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.bias = bias\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
        "                              out_channels=4 * self.hidden_dim,\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
        "\n",
        "        combined_conv = self.conv(combined)\n",
        "\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(batch_size, self.hidden_dim, self.height, self.width).cuda(),\n",
        "                torch.zeros(batch_size, self.hidden_dim, self.height, self.width).cuda())\n",
        "\n",
        "\n",
        "class ConvLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n",
        "                 batch_first=False, bias=True, return_all_layers=False):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "\n",
        "        self._check_kernel_size_consistency(kernel_size)\n",
        "\n",
        "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
        "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
        "            raise ValueError('Inconsistent list length.')\n",
        "\n",
        "        self.height, self.width = input_size\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "\n",
        "        cell_list = []\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
        "\n",
        "            cell_list.append(ConvLSTMCell(input_size=(self.height, self.width),\n",
        "                                          input_dim=cur_input_dim,\n",
        "                                          hidden_dim=self.hidden_dim[i],\n",
        "                                          kernel_size=self.kernel_size[i],\n",
        "                                          bias=self.bias))\n",
        "\n",
        "        self.cell_list = nn.ModuleList(cell_list)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_state=None):\n",
        "        \"\"\"\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_tensor: todo\n",
        "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
        "        hidden_state: todo\n",
        "            None. todo implement stateful\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        last_state_list, layer_output\n",
        "        \"\"\"\n",
        "        if not self.batch_first:\n",
        "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
        "            input_tensor=input_tensor.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        # Implement stateful ConvLSTM\n",
        "        if hidden_state is not None:\n",
        "            raise NotImplementedError()\n",
        "        else:\n",
        "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n",
        "\n",
        "        layer_output_list = []\n",
        "        last_state_list = []\n",
        "\n",
        "        seq_len = input_tensor.size(1)\n",
        "        cur_layer_input = input_tensor\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "\n",
        "            h, c = hidden_state[layer_idx]\n",
        "            output_inner = []\n",
        "            for t in range(seq_len):\n",
        "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
        "                                                 cur_state=[h, c])\n",
        "\n",
        "\n",
        "                output_inner.append(h)\n",
        "\n",
        "            layer_output = torch.stack(output_inner, dim=1)\n",
        "            cur_layer_input = layer_output\n",
        "\n",
        "            layer_output = layer_output.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "            layer_output_list.append(layer_output)\n",
        "            last_state_list.append([h, c])\n",
        "\n",
        "        if not self.return_all_layers:\n",
        "            layer_output_list = layer_output_list[-1:]\n",
        "            last_state_list = last_state_list[-1:]\n",
        "\n",
        "        return layer_output_list, last_state_list\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
        "        return init_states\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_kernel_size_consistency(kernel_size):\n",
        "        if not (isinstance(kernel_size, tuple) or\n",
        "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
        "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
        "\n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gxTptRGMLk0A"
      },
      "source": [
        "#@title train.py\n",
        "import torch\n",
        "import time\n",
        "from torchvision import transforms\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "def train(args, epoch, model, train_loader, device, optimizer, criterion):\n",
        "    since = time.time()\n",
        "    model.train()\n",
        "    for batch_idx,  sample_batched in enumerate(train_loader):\n",
        "        data, target = sample_batched['data'].to(device), sample_batched['label'].type(torch.LongTensor).to(device) # LongTensor\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Train Epoch: {} complete in {:.0f}m {:.0f}s'.format(epoch,\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "def val(args, model, val_loader, device, criterion, best_acc):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for sample_batched in val_loader:\n",
        "            data, target = sample_batched['data'].to(device), sample_batched['label'].type(torch.LongTensor).to(device)\n",
        "            output,_ = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= (len(val_loader.dataset)/args.test_batch_size)\n",
        "    val_acc = 100. * int(correct) / (len(val_loader.dataset) * label_height * label_width)\n",
        "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.5f}%)\\n'.format(\n",
        "        test_loss, int(correct), len(val_loader.dataset), val_acc))\n",
        "    torch.save(model.state_dict(), '%s.pth'%val_acc)\n",
        "\n",
        "\n",
        "def get_parameters(model, layer_name):\n",
        "    import torch.nn as nn\n",
        "    modules_skipped = (\n",
        "        nn.ReLU,\n",
        "        nn.MaxPool2d,\n",
        "        nn.Dropout2d,\n",
        "        nn.UpsamplingBilinear2d\n",
        "    )\n",
        "    for name, module in model.named_children():\n",
        "        if name in layer_name:\n",
        "            for layer in module.children():\n",
        "                if isinstance(layer, modules_skipped):\n",
        "                    continue\n",
        "                else:\n",
        "                    for parma in layer.parameters():\n",
        "                        yield parma\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "S6TMeYKtLnzJ",
        "cellView": "form"
      },
      "source": [
        "#@title Training Code\n",
        "# args = args_setting()\n",
        "# torch.manual_seed(args.seed)\n",
        "# torch.manual_seed(0)\n",
        "# use_cuda = args.cuda and torch.cuda.is_available()\n",
        "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "# # turn image into floatTensor\n",
        "# op_tranforms = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# # load data for batches, num_workers for multiprocess\n",
        "# if args.model == 'SegNet-ConvLSTM' or 'UNet-ConvLSTM':\n",
        "#     train_loader = torch.utils.data.DataLoader(\n",
        "#         RoadSequenceDatasetList(file_path=train_path, transforms=op_tranforms),\n",
        "#         batch_size=args.batch_size,shuffle=True,num_workers=data_loader_numworkers)\n",
        "#     val_loader = torch.utils.data.DataLoader(\n",
        "#         RoadSequenceDatasetList(file_path=val_path, transforms=op_tranforms),\n",
        "#         batch_size=args.test_batch_size,shuffle=True,num_workers=data_loader_numworkers)\n",
        "# else:\n",
        "#     train_loader = torch.utils.data.DataLoader(\n",
        "#         RoadSequenceDataset(file_path=train_path, transforms=op_tranforms),\n",
        "#         batch_size=args.batch_size, shuffle=True, num_workers=data_loader_numworkers)\n",
        "#     val_loader = torch.utils.data.DataLoader(\n",
        "#         RoadSequenceDataset(file_path=val_path, transforms=op_tranforms),\n",
        "#         batch_size=args.test_batch_size, shuffle=True, num_workers=data_loader_numworkers)\n",
        "\n",
        "# #load model\n",
        "# model = generate_model(args)\n",
        "\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "# # optimizer = torch.optim.Adam([\n",
        "# #     {'params': get_parameters(model, layer_name=[\"inc\", \"down1\", \"down2\", \"down3\", \"down4\"]), 'lr': args.lr * 0.0},\n",
        "# #     {'params': get_parameters(model, layer_name=[\"outc\", \"up1\", \"up2\", \"up3\", \"up4\"]), 'lr': args.lr * 0.1},\n",
        "# #     {'params': get_parameters(model, layer_name=[\"convlstm\"]), 'lr': args.lr * 1},\n",
        "# # ], lr=args.lr)\n",
        "# # optimizer = torch.optim.SGD([\n",
        "# #     {'params': get_parameters(model, layer_name=[\"conv1_block\", \"conv2_block\", \"conv3_block\", \"conv4_block\", \"conv5_block\"]), 'lr': args.lr * 0.5},\n",
        "# #     {'params': get_parameters(model, layer_name=[\"upconv5_block\", \"upconv4_block\", \"upconv3_block\", \"upconv2_block\", \"upconv1_block\"]), 'lr': args.lr * 0.33},\n",
        "# #     {'params': get_parameters(model, layer_name=[\"Conv3D_block\"]), 'lr': args.lr * 0.5},\n",
        "# # ], lr=args.lr,momentum=0.9)\n",
        "\n",
        "# scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
        "# class_weight = torch.Tensor(class_weight)\n",
        "# criterion = torch.nn.CrossEntropyLoss(weight=class_weight).to(device)\n",
        "# best_acc = 0\n",
        "\n",
        "# pretrained_dict = torch.load(pretrained_path)\n",
        "# model_dict = model.state_dict()\n",
        "\n",
        "# pretrained_dict_1 = {k: v for k, v in pretrained_dict.items() if (k in model_dict)}\n",
        "# model_dict.update(pretrained_dict_1)\n",
        "# model.load_state_dict(model_dict)\n",
        "\n",
        "# # train\n",
        "# for epoch in range(1, args.epochs+1):\n",
        "#     scheduler.step()\n",
        "#     train(args, epoch, model, train_loader, device, optimizer, criterion)\n",
        "#     val(args, model, val_loader, device, criterion, best_acc)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lprOtnNVLFXZ",
        "cellView": "form"
      },
      "source": [
        "#@title \n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.optim import lr_scheduler\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def output_result(model, test_loader, device):\n",
        "    model.eval()\n",
        "    k = 0\n",
        "    feature_dic=[]\n",
        "    with torch.no_grad():\n",
        "        for sample_batched in test_loader:\n",
        "            k+=1\n",
        "            print(k)\n",
        "            data, target = sample_batched['data'].to(device), sample_batched['label'].type(torch.LongTensor).to(device)\n",
        "            output,feature = model(data)\n",
        "            feature_dic.append(feature)\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            img = torch.squeeze(pred).cpu().unsqueeze(2).expand(-1,-1,3).numpy()*255\n",
        "            img = Image.fromarray(img.astype(np.uint8))\n",
        "\n",
        "            data = torch.squeeze(data).cpu().numpy()\n",
        "            if args.model == 'SegNet-ConvLSTM' or 'UNet-ConvLSTM':\n",
        "                data = np.transpose(data[-1], [1, 2, 0]) * 255\n",
        "            else:\n",
        "                data = np.transpose(data, [1, 2, 0]) * 255\n",
        "            data = Image.fromarray(data.astype(np.uint8))\n",
        "            rows = img.size[0]\n",
        "            cols = img.size[1]\n",
        "            for i in range(0, rows):\n",
        "                for j in range(0, cols):\n",
        "                    img2 = (img.getpixel((i, j)))\n",
        "                    if (img2[0] > 200 or img2[1] > 200 or img2[2] > 200):\n",
        "                        data.putpixel((i, j), (234, 53, 57, 255))\n",
        "            data = data.convert(\"RGB\")\n",
        "            data.save(save_path + \"%s_data.jpg\" % k)#red line on the original image\n",
        "            img.save(save_path + \"%s_pred.jpg\" % k)#prediction result\n",
        "\n",
        "def evaluate_model(model, test_loader, device, criterion):\n",
        "    model.eval()\n",
        "    i = 0\n",
        "    precision = 0.0\n",
        "    recall = 0.0\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    error=0\n",
        "    with torch.no_grad():\n",
        "        for sample_batched in test_loader:\n",
        "            i+=1\n",
        "            data, target = sample_batched['data'].to(device), sample_batched['label'].type(torch.LongTensor).to(device)\n",
        "            output, feature = model(data)\n",
        "            pred = output.max(1, keepdim=True)[1]  # 返回两个，一个是最大值另一个是最大值索引\n",
        "            img = torch.squeeze(pred).cpu().numpy()*255\n",
        "            lab = torch.squeeze(target).cpu().numpy()*255\n",
        "            img = img.astype(np.uint8)\n",
        "            lab = lab.astype(np.uint8)\n",
        "            kernel = np.uint8(np.ones((3, 3)))\n",
        "\n",
        "            #accuracy\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "            #precision,recall,f1\n",
        "            label_precision = cv2.dilate(lab, kernel)\n",
        "            pred_recall = cv2.dilate(img, kernel)\n",
        "            img = img.astype(np.int32)\n",
        "            lab = lab.astype(np.int32)\n",
        "            label_precision = label_precision.astype(np.int32)\n",
        "            pred_recall = pred_recall.astype(np.int32)\n",
        "            a = len(np.nonzero(img*label_precision)[1])\n",
        "            b = len(np.nonzero(img)[1])\n",
        "            if b==0:\n",
        "                error=error+1\n",
        "                continue\n",
        "            else:\n",
        "                precision += float(a/b)\n",
        "            c = len(np.nonzero(pred_recall*lab)[1])\n",
        "            d = len(np.nonzero(lab)[1])\n",
        "            if d==0:\n",
        "                error = error + 1\n",
        "                continue\n",
        "            else:\n",
        "                recall += float(c / d)\n",
        "            F1_measure=(2*precision*recall)/(precision+recall)\n",
        "\n",
        "    test_loss /= (len(test_loader.dataset) / args.test_batch_size)\n",
        "    test_acc = 100. * int(correct) / (len(test_loader.dataset) * label_height * label_width)\n",
        "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.5f}%)'.format(\n",
        "        test_loss, int(correct), len(test_loader.dataset), test_acc))\n",
        "\n",
        "    precision = precision / (len(test_loader.dataset) - error)\n",
        "    recall = recall / (len(test_loader.dataset) - error)\n",
        "    F1_measure = F1_measure / (len(test_loader.dataset) - error)\n",
        "    print('Precision: {:.5f}, Recall: {:.5f}, F1_measure: {:.5f}\\n'.format(precision,recall,F1_measure))\n",
        "\n",
        "\n",
        "def get_parameters(model, layer_name):\n",
        "    import torch.nn as nn\n",
        "    modules_skipped = (\n",
        "        nn.ReLU,\n",
        "        nn.MaxPool2d,\n",
        "        nn.Dropout2d,\n",
        "        nn.UpsamplingBilinear2d\n",
        "    )\n",
        "    for name, module in model.named_children():\n",
        "        if name in layer_name:\n",
        "            for layer in module.children():\n",
        "                if isinstance(layer, modules_skipped):\n",
        "                    continue\n",
        "                else:\n",
        "                    for parma in layer.parameters():\n",
        "                        yield parma"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNNylgj4LZy5",
        "cellView": "form",
        "outputId": "e3b5b8ae-6687-4703-db74-6aaa1c61c0c7"
      },
      "source": [
        "#@title\n",
        "\n",
        "args = args_setting()\n",
        "torch.manual_seed(args.seed)\n",
        "# torch.manual_seed(0)\n",
        "use_cuda = args.cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "#turn image into floatTensor\n",
        "op_tranforms = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# load data for batches, num_workers for multiprocess\n",
        "if model == 'SegNet-ConvLSTM' or 'UNet-ConvLSTM':\n",
        "    test_loader=torch.utils.data.DataLoader(\n",
        "        RoadSequenceDatasetList(file_path=test_path, transforms=op_tranforms),\n",
        "        batch_size=args.test_batch_size, shuffle=False, num_workers=1)\n",
        "else:\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        RoadSequenceDataset(file_path=test_path, transforms=op_tranforms),\n",
        "        batch_size=args.test_batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "# load model and weights\n",
        "model = generate_model(args)\n",
        "class_weight = torch.Tensor(class_weight)\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weight).to(device)\n",
        "\n",
        "pretrained_dict = torch.load(pretrained_path)\n",
        "model_dict = model.state_dict()\n",
        "pretrained_dict_1 = {k: v for k, v in pretrained_dict.items() if (k in model_dict)}\n",
        "model_dict.update(pretrained_dict_1)\n",
        "model.load_state_dict(model_dict)\n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV80pUJWL5ej",
        "outputId": "2c3532d5-0d55-4614-a7b9-39dd25164556"
      },
      "source": [
        "# output the result pictures\n",
        "output_result(model, test_loader, device)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g4FBEapDR442",
        "outputId": "33323c42-6507-400f-f2f4-f52b1bf179fd"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/robust-lane-detection-main/LaneDetectionCode'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0K3-gZIGQQe",
        "outputId": "ca215033-5bb0-4687-dc4d-bd0dad3ebc18"
      },
      "source": [
        "# calculate the values of accuracy, precision, recall, f1_measure\n",
        "evaluate_model(model, test_loader, device, criterion)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Average loss: 2.0016, Accuracy: 160345/5 (97.86682%)\n",
            "Precision: 0.78472, Recall: 0.93155, F1_measure: 0.85185\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77VfvfjWVX-K",
        "outputId": "23a4a255-288e-4307-eef0-6e48ada23828"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "  Downloading https://files.pythonhosted.org/packages/79/e7/643808913211d6c1fc96a3a4333bf4c9276858fab00bcafaf98ea58a97be/torchviz-0.0.2.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.8.1+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (1.19.5)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-cp37-none-any.whl size=4152 sha256=0cfdf3ece69014651255503e7c2187a3225bdd2d5b769ea5041e33293e806157\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/26/58/026ffd533dbe8b3972eb423da9c7949beca68d1c98ed9e8624\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59gbzQb7VWWt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}